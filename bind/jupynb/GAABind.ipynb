{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82c46083-d92b-4560-b262-4a7ab46f0802",
   "metadata": {},
   "source": [
    "### Standardize Workflow -- Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebe7feb0-a00a-4749-a0c3-397da2302fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser, Namespace, FileType\n",
    "import os\n",
    "os.chdir(\"/scratch/phjiang/gbind/\") #pathto directory\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import argparse\n",
    "from glob import glob\n",
    "from multiprocessing import Pool\n",
    "import GAABind\n",
    "from GAABind.data.feature_utils import get_ligand_info, get_protein_info, get_chem_feats, read_mol, get_coords\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "from GAABind.utils import set_global_seed\n",
    "from GAABind.data.graph_dataset import DockingTestDataset\n",
    "from GAABind.data.collator import collator_test_3d\n",
    "from GAABind.option import set_args\n",
    "from GAABind.models.DockingPoseModel import DockingPoseModel\n",
    "from GAABind.docking.docking_utils import (\n",
    "    docking_data_pre,\n",
    "    ensemble_iterations,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import time\n",
    "\n",
    "def load_model():\n",
    "    # load model\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    parser = set_args()\n",
    "    args = parser.parse_args(args=[])\n",
    "    set_global_seed(args.seed)\n",
    "    ckpt_path = './GAABind/saved_model/best_epoch.pt'\n",
    "    state_dict = torch.load(ckpt_path, map_location='cpu')\n",
    "    new_state_dict = dict()\n",
    "    for key in state_dict.keys():\n",
    "        layer_name = key[7:]\n",
    "        new_state_dict[layer_name] = state_dict[key]\n",
    "    \n",
    "    model = DockingPoseModel(args).to(device)\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    return model, args, device\n",
    "\n",
    "\n",
    "def most_recent(directory):\n",
    "    a = os.listdir(directory)\n",
    "    b = [f\"{directory}/{x}\" for x in a if \".\" not in x]\n",
    "    latest = max(b, key=os.path.getmtime)\n",
    "    return latest\n",
    "\n",
    "def checkprevProg(inDir, outDir):\n",
    "    path = \"\"\n",
    "    output = []\n",
    "    \n",
    "    if(\"part\" in inDir):\n",
    "        path = inDir.split(\"part\")[0]\n",
    "    else:\n",
    "        path = inDir\n",
    "    try:\n",
    "        evaluated = os.listdir(f\"./outputs/{outDir}\")\n",
    "    except:\n",
    "        print(\"no previous progress\")\n",
    "        os.mkdir(f\"./outputs/{outDir}\")\n",
    "        return [x for x in os.listdir(inDir) if \".\" not in x]\n",
    "    else:\n",
    "        paths = [x for x in os.listdir(inDir) if \".\" not in x]\n",
    "        if(len(evaluated) != 0):\n",
    "            if(os.path.isdir(path)):\n",
    "                output = [y for y in paths if y not in evaluated]\n",
    "                output.append(most_recent(inDir))\n",
    "                #print(paths, np.sort(np.array(evaluated)))\n",
    "            else:\n",
    "                output = paths\n",
    "        else:\n",
    "            output = paths\n",
    "            \n",
    "        return output\n",
    "    \n",
    "\n",
    "def runbind(inDir, outDir):\n",
    "    start_time = time.time()\n",
    "    #input_paths = ['sys31']  #(Example)\n",
    "    ptemp = os.listdir(inDir)\n",
    "    input_paths = checkprevProg(inDir, outDir)\n",
    "    #print(input_paths)\n",
    "    for i,x in enumerate(tqdm(input_paths)):\n",
    "        #print(f\"\\n\\tBegan evaluation of ligand: {input_paths[i]}\\n\")\n",
    "        passed = True\n",
    "        input_path = f\"./{inDir}/{input_paths[i]}\"\n",
    "        name = os.path.basename(input_path)\n",
    "        mol_file = glob(f'{input_path}/ligand.txt')[0]\n",
    "        pocket_file = glob(f'{input_path}/pocket.txt')[0]\n",
    "        \n",
    "        pro_file = glob(f'{input_path}/protein.pdb')[0]\n",
    "        #pocket_file = f'./templates/pocket2.txt'\n",
    "        #Hard code file location -- decreases unnecessary file read times (?)\n",
    "        \n",
    "        poc_res = open(pocket_file).read().splitlines()\n",
    "        #print('using dataset path: ', mol_file, pro_file, pocket_file)\n",
    "        #print('using the following reisudes as target pocket: ', ','.join(poc_res))\n",
    "        \n",
    "        if not os.path.exists(f'./outputs/{outDir}'):\n",
    "            os.makedirs(f\"./outputs/{outDir}\")\n",
    "        \n",
    "        output_dir = f'./outputs/{outDir}/{input_paths[i]}' #replace the save path by your own\n",
    "    \n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "        output_path = os.path.join(output_dir, f'{name}.pkl')\n",
    "        new_data = {}\n",
    "        try:\n",
    "            input_mol = read_mol(mol_file)\n",
    "        except:\n",
    "            print(\"\\tfailed to read ligand from file -- RDKit Error\")\n",
    "            passed = False\n",
    "        else:\n",
    "            try:\n",
    "                mol, smiles, coordinate_list = get_coords(input_mol)\n",
    "            except:\n",
    "                print(f'\\tgenerate input ligand coords failed for {name}')\n",
    "                passed = False\n",
    "            \n",
    "        if passed:\n",
    "            lig_atoms, lig_atom_feats, lig_edges, lig_bonds = get_ligand_info(mol)\n",
    "            poc_pos, poc_atoms, poc_atom_feats, poc_edges, poc_bonds = get_protein_info(pro_file, poc_res)\n",
    "            \n",
    "            new_data.update({'atoms': lig_atoms, 'coordinates': coordinate_list, 'pocket_atoms': poc_atoms,\n",
    "                            'pocket_coordinates': poc_pos, 'smi': smiles, 'pocket': name,'lig_feats': lig_atom_feats,\n",
    "                            'lig_bonds': lig_edges, 'lig_bonds_feats': lig_bonds, 'poc_feats': poc_atom_feats, \n",
    "                            'poc_bonds': poc_edges, 'poc_bonds_feats': poc_bonds, 'mol': mol})\n",
    "\n",
    "            \n",
    "            pass2 = True\n",
    "            try:\n",
    "                new_data = get_chem_feats(new_data)\n",
    "            except:\n",
    "                print(f\"\\t failed to generate chemical features for {name}\")\n",
    "                pass2 = False\n",
    "            else:\n",
    "                f_out = open(output_path, 'wb')\n",
    "                pickle.dump(new_data, f_out)\n",
    "                f_out.close()\n",
    "        \n",
    "                # load model\n",
    "                model, args, device = load_model()\n",
    "        \n",
    "                inference_save_path = os.path.join(output_dir, f'{name}.pkl')   #define the save path of inference\n",
    "                test_dataset = DockingTestDataset(output_dir, args.conf_size)\n",
    "                test_dataloader = DataLoader(test_dataset, batch_size=args.batch_size, num_workers=args.num_workers, shuffle=False, collate_fn=collator_test_3d)\n",
    "                \n",
    "                outputs = []\n",
    "                with torch.no_grad():\n",
    "                    model.eval()\n",
    "                    for batch in test_dataloader:\n",
    "                        for dicts in batch[:2]:\n",
    "                            for key in dicts.keys():\n",
    "                                dicts[key] = dicts[key].to(device)\n",
    "                \n",
    "                        with torch.cuda.amp.autocast():\n",
    "                            pred = model(batch)\n",
    "                \n",
    "                        mol_token_atoms = batch[0]['x'][:,:,0]\n",
    "                        poc_token_atoms = batch[1]['x'][:,:,0]\n",
    "                        poc_coords = batch[1]['pos']\n",
    "                \n",
    "                        logging_output = {}\n",
    "                \n",
    "                        logging_output[\"smi_name\"] = batch[2]['smi_list']\n",
    "                        logging_output[\"pocket_name\"] = batch[2]['pocket_list']\n",
    "                        logging_output['mol'] = batch[2]['mol']\n",
    "                        logging_output[\"cross_distance_predict\"] = pred[0].data.detach().cpu().permute(0, 2, 1)\n",
    "                        logging_output[\"holo_distance_predict\"] = pred[1].data.detach().cpu()\n",
    "                        logging_output[\"atoms\"] = mol_token_atoms.data.detach().cpu()\n",
    "                        logging_output[\"pocket_atoms\"] = poc_token_atoms.data.detach().cpu()\n",
    "                        logging_output[\"holo_center_coordinates\"] = batch[2]['holo_center_list']\n",
    "                        logging_output[\"pocket_coordinates\"] = poc_coords.data.detach().cpu()\n",
    "                        logging_output['pred_affinity'] = pred[-1].data.detach().cpu()\n",
    "                        outputs.append(logging_output)\n",
    "                        #print(logging_output['pred_affinity'])\n",
    "                \n",
    "                    pickle.dump(outputs, open(inference_save_path, \"wb\"))\n",
    "        \n",
    "                mol_list, smi_list, pocket_list, pocket_coords_list, distance_predict_list, holo_distance_predict_list,\\\n",
    "                        holo_center_coords_list, pred_affi_list = docking_data_pre(inference_save_path)\n",
    "                iterations = ensemble_iterations(mol_list, smi_list, pocket_list, pocket_coords_list, distance_predict_list,\\\n",
    "                                                     holo_distance_predict_list, holo_center_coords_list, pred_affi_list)\n",
    "        \n",
    "                cache_dir = os.path.join(output_dir, \"cache\")\n",
    "                os.makedirs(cache_dir, exist_ok=True)\n",
    "                cache_file = os.path.join(cache_dir, f'{name}.pkl')\n",
    "                \n",
    "                pd.to_pickle(next(iterations), cache_file)\n",
    "                \n",
    "                output_ligand_path = os.path.join(output_dir, name)\n",
    "                cmd = \"python ./GAABind/docking/coordinate_model.py --input {}  --output-path {}\".format(cache_file, output_ligand_path)\n",
    "                os.system(cmd)\n",
    "                print(f'Prediction fininshed for {input_paths[i]}.\\n')\n",
    "    print(\"--- Batch complete. Time Elapsed: %5.5fs---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5515a20-3b61-4c7f-b9c6-21f687af3f3b",
   "metadata": {},
   "source": [
    "### Run Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f30c8737-148a-4ce8-886e-c689bf8a071a",
   "metadata": {},
   "outputs": [],
   "source": [
    "runbind(f\"Your Input Path Here\", f\"Your Output Path Here -- recommend same name for both, they will save to different directories\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gbind",
   "language": "python",
   "name": "gbind"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
